from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, TrainingArguments, Trainer, DataCollatorForSeq2Seq
from datasets import load_dataset, DatasetDict

# 1. Chargement du modèle et du tokenizer
model_name = "google/flan-t5-small"
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)
tokenizer = AutoTokenizer.from_pretrained(model_name)

# 2. Chargement du dataset
dataset = load_dataset("gsm8k", "main", split="train")

# 3. Prétraitement des données
def preprocess_data(examples):
    inputs = [q.strip() for q in examples['question']]
    targets = [a.strip() for a in examples['answer']]
    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding="max_length")
    labels = tokenizer(targets, max_length=256, truncation=True, padding="max_length").input_ids
    model_inputs["labels"] = labels
    return model_inputs

tokenized_dataset = dataset.map(preprocess_data, batched=True)

# 4. Création des ensembles d'entraînement et de validation
split_dataset = tokenized_dataset.train_test_split(test_size=0.2)
dataset = DatasetDict({
    'train': split_dataset['train'],
    'validation': split_dataset['test']
})

# 5. Définition du collator pour le padding dynamique
data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)

# 6. Configuration des arguments d'entraînement
training_args = TrainingArguments(
    output_dir="./results",
    evaluation_strategy="epoch",
    learning_rate=2e-5,
    per_device_train_batch_size=4,
    per_device_eval_batch_size=4,
    num_train_epochs=3,
    weight_decay=0.01,
)

# 7. Initialisation du Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=dataset["train"],
    eval_dataset=dataset["validation"],
    data_collator=data_collator
)

# 8. Entraînement du modèle
trainer.train()

# 9. Sauvegarde du modèle fine-tuné sous un autre nom
output_dir = "./fine_tuned_models/main_model"
model.save_pretrained(output_dir)
tokenizer.save_pretrained(output_dir)

# 10. Fonction de génération de réponse
def generate_answer(question):
    inputs = tokenizer(question, return_tensors="pt", truncation=True, padding=True)
    outputs = model.generate(**inputs, max_length=150, num_beams=4, early_stopping=True)
    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return answer

